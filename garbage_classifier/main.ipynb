{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于深度学习的机器视觉 - 垃圾分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 实验介绍\n",
    "## 1.1 实验背景\n",
    "\n",
    "自今年 7 月 1 日起，上海市将正式实施 《上海市生活垃圾管理条例》。  \n",
    "垃圾分类，看似是微不足道的“小事”，实则关系到13亿多人生活环境的改善，理应大力提倡。  \n",
    "垃圾识别分类数据集中包括玻璃 (glass) 、硬纸板 (cardboard) 、金属 (metal) 、纸 (paper) 、塑料 (plastic) 、一般垃圾 (trash) ，共6个类别。    \n",
    "生活垃圾由于种类繁多，具体分类缺乏统一标准，大多人在实际操作时会“选择困难”，基于深度学习技术建立准确的分类模型，利用技术手段改善人居环境。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 实验要求\n",
    "a）建立深度神经网络模型，并尽可能将其调到最佳状态。   \n",
    "b）绘制深度神经网络模型图、绘制并分析学习曲线。  \n",
    "c）用准确率等指标对模型进行评估。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 实验环境\n",
    "可以使用基于 Python 的 OpenCV 库进行图像相关处理，使用 Numpy 库进行相关数值运算，使用 Keras 等框架建立深度学习模型等。\n",
    "\n",
    "## 1.4 注意事项\n",
    "+ Python 与 Python Package 的使用方式，可在右侧 `API文档` 中查阅。\n",
    "+ 当右上角的『Python 3』长时间指示为运行中的时候，造成代码无法执行时，可以重新启动 Kernel 解决（左上角『Kernel』-『Restart Kernel』）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 参考资料\n",
    "OpenCV：https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html  \n",
    "Numpy：https://www.numpy.org/  \n",
    "Keras: https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.实验内容\n",
    "## 2.1 介绍数据集\n",
    "\n",
    "该数据集包含了 2507 个生活垃圾图片。数据集的创建者将垃圾分为了 6 个类别，分别是：\n",
    "\n",
    "|序号|中文名|英文名|数据集大小|\n",
    "|--|--|--|--|\n",
    "|1|玻璃|glass| 497 |\n",
    "|2|纸|paper| 590 |\n",
    "|3|硬纸板|cardboard| 400 |\n",
    "|4|塑料|plastic| 479 |\n",
    "|5|金属|metal|407 |\n",
    "|6|一般垃圾|trash|134 |\n",
    "\n",
    "+ 物品都是放在白板上在日光/室内光源下拍摄的，压缩后的尺寸为 512 * 384\n",
    "\n",
    "导入数据集成功后路径：  \n",
    "data_path = \"./datasets/la1ji1fe1nle4ishu4ju4ji22-momodel/dataset-resized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关包\n",
    "import glob, os\n",
    "\n",
    "# 数据集路径\n",
    "data_path = \"./datasets/la1ji1fe1nle4ishu4ju4ji22-momodel/dataset-resized\"\n",
    "\n",
    "# 获取数据名称列表\n",
    "img_list = glob.glob(os.path.join(data_path, '*/*.jpg'))\n",
    "\n",
    "# 打印数据集总量\n",
    "print(\"数据集总数量:\", len(img_list))\n",
    "print(\"数据路径和名称:\",img_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集总共有 2507 张图片，现在随机展示其中的 6 张图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 从数据名称列表 img_list 中随机选取 6 个。\n",
    "for i, img_path in enumerate(random.sample(img_list, 6)):\n",
    "    \n",
    "    # 读取图片\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    # 将图片从 BGR 模式转为 RGB 模式\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 将窗口设置为 2 行 3 列 6个子图\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    \n",
    "    # 展示图片\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    # 不显示坐标尺寸\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 获取图像形状    \n",
    "img.shape 可以获得图像的形状，返回值是一个包含行数，列数，通道数的元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机选取一张图片\n",
    "path = random.sample(img_list, 1)\n",
    "\n",
    "# 读取图片\n",
    "img = cv2.imread(path[0])\n",
    "\n",
    "# 将图片从 BGR 模式转为 RGB 模式\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 获取图片的形状\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 图片预处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图片生成器 [ImageDataGenerator](https://keras.io/preprocessing/image/): keras.preprocessing.image 模块中的图片生成器，主要用以生成一个 batch 的图像数据，支持实时数据提升。训练时该函数会无限生成数据，直到达到规定的 epoch 次数为止。同时也可以在 batch 中对数据进行增强，扩充数据集大小，增强模型的泛化能力，比如进行旋转，变形，归一化等等。\n",
    "    \n",
    "图片生成器的主要方法：\n",
    "+ fit(x, augment=False, rounds=1)：计算依赖于数据的变换所需要的统计信息(均值方差等)。  \n",
    "\n",
    "+ flow(self, X, y, batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png')：接收 Numpy 数组和标签为参数,生成经过数据提升或标准化后的 batch 数据，并在一个无限循环中不断的返回 batch 数据。  \n",
    "\n",
    "\n",
    "+ flow_from_directory(directory): 以文件夹路径为参数，会从路径推测 label，生成经过数据提升/归一化后的数据，在一个无限循环中无限产生 batch 数据。\n",
    "\n",
    "英文参考链接：https://keras.io/preprocessing/image/  \n",
    "中文参考链接：https://keras-cn.readthedocs.io/en/latest/preprocessing/image/\n",
    "\n",
    "以上只是对图片生成器进行简单的介绍，详细信息请参考中英文链接。  \n",
    "根据上面的介绍和我们数据集的特性，我们主要运用 `ImageDataGenerator()` 和 `flow_from_directory()` 方法。我们将数据处理过程封装成为一个函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 导入图片生成器\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "def processing_data(data_path, height, width, batch_size=32, validation_split=0.1):\n",
    "    \"\"\"\n",
    "    数据处理\n",
    "    :param data_path: 带有子目录的数据集路径\n",
    "    :param height: 图像形状的行数\n",
    "    :param width: 图像形状的列数\n",
    "    :param batch_size: batch 数据的大小，整数，默认32。\n",
    "    :param validation_split: 在 0 和 1 之间浮动。用作测试集的训练数据的比例，默认0.1。\n",
    "    :return: train_generator, validation_generator: 处理后的训练集数据、验证集数据\n",
    "    \"\"\"\n",
    "\n",
    "    train_data = ImageDataGenerator(\n",
    "            # 对图片的每个像素值均乘上这个放缩因子，把像素值放缩到0和1之间有利于模型的收敛\n",
    "            rescale=1. / 255,  \n",
    "            # 浮点数，剪切强度（逆时针方向的剪切变换角度）\n",
    "            shear_range=0.1,  \n",
    "            # 随机缩放的幅度，若为浮点数，则相当于[lower,upper] = [1 - zoom_range, 1+zoom_range]\n",
    "            zoom_range=0.1,\n",
    "            # 浮点数，图片宽度的某个比例，数据提升时图片水平偏移的幅度\n",
    "            width_shift_range=0.1,\n",
    "            # 浮点数，图片高度的某个比例，数据提升时图片竖直偏移的幅度\n",
    "            height_shift_range=0.1,\n",
    "            # 布尔值，进行随机水平翻转\n",
    "            horizontal_flip=True,\n",
    "            # 布尔值，进行随机竖直翻转\n",
    "            vertical_flip=True,\n",
    "            # 在 0 和 1 之间浮动。用作验证集的训练数据的比例\n",
    "            validation_split=validation_split  \n",
    "    )\n",
    "\n",
    "    # 接下来生成测试集，可以参考训练集的写法\n",
    "    validation_data = ImageDataGenerator(\n",
    "            rescale=1. / 255,\n",
    "            validation_split=validation_split)\n",
    "\n",
    "    train_generator = train_data.flow_from_directory(\n",
    "            # 提供的路径下面需要有子目录\n",
    "            data_path, \n",
    "            # 整数元组 (height, width)，默认：(256, 256)。 所有的图像将被调整到的尺寸。\n",
    "            target_size=(height, width),\n",
    "            # 一批数据的大小\n",
    "            batch_size=batch_size,\n",
    "            # \"categorical\", \"binary\", \"sparse\", \"input\" 或 None 之一。\n",
    "            # 默认：\"categorical\",返回one-hot 编码标签。\n",
    "            class_mode='categorical',\n",
    "            # 数据子集 (\"training\" 或 \"validation\")\n",
    "            subset='training', \n",
    "            seed=0)\n",
    "    validation_generator = validation_data.flow_from_directory(\n",
    "            data_path,\n",
    "            target_size=(height, width),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='categorical',\n",
    "            subset='validation',\n",
    "            seed=0)\n",
    "\n",
    "    return train_generator, validation_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据路径\n",
    "data_path = \"./datasets/la1ji1fe1nle4ishu4ju4ji22-momodel/dataset-resized\"\n",
    "\n",
    "# 图像数据的行数和列数\n",
    "height, width = 384, 512\n",
    "\n",
    "# 获取训练数据和验证数据集\n",
    "train_generator, validation_generator = processing_data(data_path, height, width)\n",
    "\n",
    "# 通过属性class_indices可获得文件夹名与类的序号的对应字典。 (类别的顺序将按照字母表顺序映射到标签值)。\n",
    "labels = train_generator.class_indices\n",
    "print(labels)\n",
    "\n",
    "# 转换为类的序号与文件夹名对应的字典\n",
    "labels = dict((v, k) for k, v in labels.items())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 采用 Keras 建立一个简单的深度神经网络模型\n",
    "通过 Keras 构建深度学习模型的步骤如下：\n",
    "+ 定义模型——创建一个模型并添加配置层\n",
    "+ 编译模型——指定损失函数和优化器，并调用模型的 compile() 函数，完成模型编译。\n",
    "+ 训练模型——通过调用模型的 fit() 函数来训练模型。\n",
    "+ 模型预测——调用模型的 evaluate()或者 predict() 等函数对新数据进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 常见的创建模型方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 的核心数据结构是 model，一种组织网络层的方式。最简单的模型是 [Sequential 顺序模型](https://keras.io/getting-started/sequential-model-guide/)，它由多个网络层线性堆叠。对于更复杂的结构，你应该使用 Keras 函数式 API，它允许构建任意的神经网络图。下面先来看看 Sequential 顺序模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 方式一: 使用 .add() 方法将各层添加到模型中\n",
    "# 导入相关包\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# 选择模型，选择序贯模型（Sequential())\n",
    "model = Sequential()\n",
    "\n",
    "# 构建网络层\n",
    "# 添加全连接层，输入784维,输出空间维度32\n",
    "model.add(Dense(32, input_shape=(784,)))\n",
    "\n",
    "# 添加激活层，激活函数是 relu\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 打印模型概况\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 方式二：网络层实例的列表构建序贯模型\n",
    "# 导入相关的包\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# 选择模型，选择序贯模型（Sequential())\n",
    "# 通过将网络层实例的列表传递给 Sequential 的构造器，来创建一个 Sequential 模型\n",
    "model = Sequential([\n",
    "    Dense(32, input_shape=(784,)),\n",
    "    Activation('relu')\n",
    "])\n",
    "\n",
    "# 打印模型概况\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用 Keras 函数式模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 方式三：函数式模型\n",
    "# 导入相关的包\n",
    "from keras.layers import Input, Dense,Activation\n",
    "from keras.models import Model\n",
    "\n",
    "# 输入层，返回一个张量 tensor\n",
    "inputs = Input(shape=(784,))\n",
    "\n",
    "# 全连接层，返回一个张量\n",
    "output_1 = Dense(32)(inputs)\n",
    "\n",
    "# 激活函数层\n",
    "predictions= Activation(activation='relu')(output_1)\n",
    "\n",
    "# 创建一个模型，包含输入层、全连接层和激活层\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# 打印模型概况\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 建立深度学习模型\n",
    "\n",
    "+ 创建一个简单的深度接神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 导入相关包\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "\n",
    "def dnn_model(input_shape, train_generator, validation_generator, model_save_path='results/dnn.h5',\n",
    "              log_dir=\"results/logs/\"):\n",
    "    \"\"\"\n",
    "    该函数实现 Keras 创建深度学习模型的过程\n",
    "    :param input_shape: 模型数据形状大小，比如:input_shape=(384, 512, 3)\n",
    "    :param train_generator: 训练集\n",
    "    :param validation_generator: 验证集\n",
    "    :param model_save_path: 保存模型的路径\n",
    "    :param log_dir: 保存模型日志路径\n",
    "    :return: 返回已经训练好的模型\n",
    "    \"\"\"\n",
    "    # Input 用于实例化 Keras 张量。\n",
    "    # shape: 一个尺寸元组（整数），不包含批量大小。 例如，shape=(32,) 表明期望的输入是按批次的 32 维向量。\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # 将输入展平\n",
    "    dnn = Flatten()(inputs)\n",
    "\n",
    "    # Dense 全连接层  实现以下操作：output = activation(dot(input, kernel) + bias)\n",
    "    # 其中 activation 是按逐个元素计算的激活函数，kernel 是由网络层创建的权值矩阵，\n",
    "    # 以及 bias 是其创建的偏置向量 (只在 use_bias 为 True 时才有用)。\n",
    "    dnn = Dense(6)(dnn)\n",
    "    # 批量标准化层: 在每一个批次的数据中标准化前一层的激活项， 即应用一个维持激活项平均值接近 0，标准差接近 1 的转换。\n",
    "    # axis: 整数，需要标准化的轴 （通常是特征轴）。默认值是 -1\n",
    "    dnn = BatchNormalization(axis=-1)(dnn)\n",
    "    # 将激活函数,输出尺寸与输入尺寸一样，激活函数可以是'softmax'、'sigmoid'等\n",
    "    dnn = Activation('sigmoid')(dnn)\n",
    "    # Dropout 包括在训练中每次更新时，将输入单元的按比率随机设置为 0, 这有助于防止过拟合。\n",
    "    # rate: 在 0 和 1 之间浮动。需要丢弃的输入比例。\n",
    "    dnn = Dropout(0.25)(dnn)\n",
    "\n",
    "    dnn = Dense(12)(dnn)\n",
    "    dnn = BatchNormalization(axis=-1)(dnn)\n",
    "    dnn = Activation('relu')(dnn)\n",
    "    dnn = Dropout(0.5)(dnn)\n",
    "\n",
    "    dnn = Dense(6)(dnn)\n",
    "    dnn = BatchNormalization(axis=-1)(dnn)\n",
    "    dnn = Activation('softmax')(dnn)\n",
    "\n",
    "    outputs = dnn\n",
    "\n",
    "    # 生成一个函数型模型\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    # 编译模型, 采用 compile 函数: https://keras.io/models/model/#compile\n",
    "    model.compile(\n",
    "            # 是优化器, 主要有Adam、sgd、rmsprop等方式。\n",
    "            optimizer='Adam',\n",
    "            # 损失函数,多分类采用 categorical_crossentropy\n",
    "            loss='categorical_crossentropy',\n",
    "            # 是除了损失函数值之外的特定指标, 分类问题一般都是准确率\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "    # 可视化，TensorBoard 是由 Tensorflow 提供的一个可视化工具。\n",
    "    tensorboard = TensorBoard(log_dir)\n",
    "\n",
    "    # 训练模型, fit_generator函数:https://keras.io/models/model/#fit_generator\n",
    "    # 利用Python的生成器，逐个生成数据的batch并进行训练。\n",
    "    # callbacks: 实例列表。在训练时调用的一系列回调。详见 https://keras.io/callbacks/。\n",
    "    d = model.fit_generator(\n",
    "            # 一个生成器或 Sequence 对象的实例\n",
    "            generator=train_generator,\n",
    "            # epochs: 整数，数据的迭代总轮数。\n",
    "            epochs=5,\n",
    "            # 一个epoch包含的步数,通常应该等于你的数据集的样本数量除以批量大小。\n",
    "            steps_per_epoch=2259 // 32,\n",
    "            # 验证集\n",
    "            validation_data=validation_generator,\n",
    "            # 在验证集上,一个epoch包含的步数,通常应该等于你的数据集的样本数量除以批量大小。\n",
    "            validation_steps=248 // 32,\n",
    "            callbacks=[tensorboard])\n",
    "    # 模型保存\n",
    "    model.save(model_save_path)\n",
    "\n",
    "    return d, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 模型训练过程和模型概况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始时间\n",
    "start = time.time()\n",
    "\n",
    "# 数据预处理\n",
    "data_path = \"./datasets/la1ji1fe1nle4ishu4ju4ji22-momodel/dataset-resized\"\n",
    "\n",
    "# 图像数据的行数和列数\n",
    "height, width = 384, 512\n",
    "\n",
    "# 获取训练数据和验证数据\n",
    "train_generator, validation_generator = processing_data(data_path, height, width)\n",
    "\n",
    "# 定义模型输入大小\n",
    "input_shape=(384, 512, 3)\n",
    "\n",
    "# 训练模型，获取训练过程和训练后的模型\n",
    "res,model = dnn_model(input_shape, train_generator, validation_generator)\n",
    "\n",
    "# 打印模型概况和模型训练总数长\n",
    "model.summary()\n",
    "print(\"模型训练总时长：\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 模型训练过程图形化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "def plot_training_history(res):\n",
    "    \"\"\"\n",
    "    绘制模型的训练结果\n",
    "    :param res: 模型的训练结果\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 绘制模型训练过程的损失和平均损失\n",
    "    # 绘制模型训练过程的损失值曲线，标签是 loss\n",
    "    plt.plot(res.history['loss'], label='loss')\n",
    "    \n",
    "    # 绘制模型训练过程中的平均损失曲线，标签是 val_loss\n",
    "    plt.plot(res.history['val_loss'], label='val_loss')\n",
    "    \n",
    "    # 绘制图例,展示出每个数据对应的图像名称和图例的放置位置\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    # 展示图片\n",
    "    plt.show()\n",
    "\n",
    "    # 绘制模型训练过程中的的准确率和平均准确率\n",
    "    # 绘制模型训练过程中的准确率曲线，标签是 acc\n",
    "    plt.plot(res.history['accuracy'], label='accuracy')\n",
    "    \n",
    "    # 绘制模型训练过程中的平均准确率曲线，标签是 val_acc\n",
    "    plt.plot(res.history['val_accuracy'], label='val_accuracy')\n",
    "    \n",
    "    # 绘制图例,展示出每个数据对应的图像名称，图例的放置位置为默认值。\n",
    "    plt.legend()\n",
    "    \n",
    "    # 展示图片\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制模型训练过程曲线\n",
    "plot_training_history(res) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 加载模型和模型评估\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "def load_and_model_prediction(validation_generator):\n",
    "    \"\"\"\n",
    "    加载模型和模型评估，打印验证集的 loss 和准确度\n",
    "    :param validation_generator: 预测数据\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # 加载模型\n",
    "    model = load_model('results/dnn.h5')\n",
    "    # 获取验证集的 loss 和 accuracy\n",
    "    loss, accuracy = model.evaluate_generator(validation_generator)\n",
    "    print(\"\\nLoss: %.2f, Accuracy: %.2f%%\" % (loss, accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印模型评估的结果\n",
    "load_and_model_prediction(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 模型预测和展示模型预测结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "def plot_load_and_model_prediction(validation_generator, labels):\n",
    "    \"\"\"\n",
    "    加载模型、模型预测并展示模型预测结果等\n",
    "    :param validation_generator: 预测数据\n",
    "    :param labels: 数据标签\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 加载模型\n",
    "    model = load_model('results/dnn.h5')\n",
    "\n",
    "    # 测试集数据与标签\n",
    "    test_x, test_y = validation_generator.__getitem__(2)\n",
    "\n",
    "    # 预测值\n",
    "    preds = model.predict(test_x)\n",
    "\n",
    "    # 绘制预测图像的预测值和真实值，定义画布\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    for i in range(16):\n",
    "        # 绘制各个子图\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "\n",
    "        # 图片名称\n",
    "        plt.title(\n",
    "                'pred:%s / truth:%s' % (labels[np.argmax(preds[i])], labels[np.argmax(test_y[i])]))\n",
    "\n",
    "        # 展示图片\n",
    "        plt.imshow(test_x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 展示模型预测结果\n",
    "plot_load_and_model_prediction(validation_generator, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 加载模型并预测输入图片的类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_and_predict(img):\n",
    "    \"\"\"\n",
    "    加载模型并预测一张图片的类别\n",
    "    :param img: PIL.Image 对象\n",
    "    :return: string, 模型识别图片的类别, \n",
    "            共 'cardboard','glass','metal','paper','plastic','trash' 6 个类别\n",
    "    \"\"\"\n",
    "    # 加载模型, 默认'results/dnn.h5',请填写你的最佳模型\n",
    "    model_path = 'results/dnn.h5'\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # 把图片转换成为numpy数组\n",
    "    img = image.img_to_array(img)\n",
    "    \n",
    "    # 图片放缩\n",
    "    img = 1.0/255 * img\n",
    "\n",
    "    # expand_dims的作用是把img.shape转换成(1, img.shape[0], img.shape[1], img.shape[2])\n",
    "    x = np.expand_dims(img, axis=0)\n",
    "\n",
    "    # 模型预测\n",
    "    y = model.predict(x)\n",
    "\n",
    "    # 获取labels\n",
    "    labels = {0: 'cardboard', 1: 'glass', 2: 'metal', 3: 'paper', 4: 'plastic', 5: 'trash'}\n",
    "\n",
    "    # 获取输入图片的类别\n",
    "    y_predict = labels[np.argmax(y)]\n",
    "\n",
    "    # 返回图片的类别\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "# 输入图片路径和名称\n",
    "file_path = 'test.jpg'\n",
    "\n",
    "# 打印该张图片的类别\n",
    "img = image.load_img(file_path)\n",
    "\n",
    "print(load_and_predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 作业\n",
    "\n",
    "通过对以上步骤流程的了解，相信大家对深度学习有了深刻的认识，但是模型比较简单，准确率也不高，大家可以试着写自己的深度学习模型，并将其调到最佳状态。在训练模型等过程中如果需要**保存数据、模型**等请写到 **results** 文件夹。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 训练深度学习模型\n",
    "\n",
    "深度学习模型训练流程, 包含数据处理、创建模型、训练模型、模型保存、评价模型等。  \n",
    "如果对训练出来的模型不满意, 你可以通过调整模型的参数等方法重新训练模型, 直至训练出你满意的模型。  \n",
    "如果你对自己训练出来的模型非常满意, 则可以提交作业!  \n",
    "\n",
    "注意：\n",
    "\n",
    "1. 你可以在我们准好的接口中实现深度学习模型（若使用可以修改函数接口），也可以自己实现深度学习模型。\n",
    "2. 写好代码后可以在 Py 文件中使用 GPU 进行模型训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================  实现自己的深度学习模型代码答题区域  ===========================================\n",
    "\n",
    "双击下方区域开始编写  **数据处理**、**创建模型**、**训练模型**、**保存模型**  和  **评估模型**  等部分的代码，请勿在别的位置作答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from garbage_classifier import data_feed\n",
    "from garbage_classifier import model\n",
    "\n",
    "def processing_data(data_path):\n",
    "    \"\"\"\n",
    "    数据处理\n",
    "    :param data_path: 数据集路径\n",
    "    :return: train, test:处理后的训练集数据、测试集数据\n",
    "    \"\"\"\n",
    "    # -------------------------- 实现数据处理部分代码 ----------------------------\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    train_data, test_data = data_feed.data_feed(data_path, (384, 512))\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def model(train_data, test_data, save_model_path):\n",
    "    \"\"\"\n",
    "    创建、训练和保存深度学习模型\n",
    "    :param train_data: 训练集数据\n",
    "    :param test_data: 测试集数据\n",
    "    :param save_model_path: 保存模型的路径和名称\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # --------------------- 实现模型创建、训练和保存等部分的代码 ---------------------\n",
    "    classification_model = model.resnet_50_pretrained((384, 512), 6)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(), \n",
    "        loss=keras.losses.categorical_crossentropy, \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(\n",
    "        train_data,\n",
    "        validation_data=test_data,\n",
    "        epochs=32,\n",
    "        steps_per_epoch=512,\n",
    "        validation_steps=64,\n",
    "        callbacks=[\n",
    "            keras.callbacks.LearningRateScheduler(lambda ei, lr: lr / 2.0 if ei > 0 and (ei / 8).is_integer() else lr)\n",
    "        ]\n",
    "    )\n",
    "    # 保存模型（请写好保存模型的路径及名称）\n",
    "    # -------------------------------------------------------------------------\n",
    "    model.save_weights(save_model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_mode(test_data, save_model_path):\n",
    "    \"\"\"\n",
    "    加载模型和评估模型\n",
    "    可以实现，比如: 模型训练过程中的学习曲线，测试集数据的loss值、准确率及混淆矩阵等评价指标！\n",
    "    主要步骤:\n",
    "        1.加载模型(请填写你训练好的最佳模型),\n",
    "        2.对自己训练的模型进行评估\n",
    "\n",
    "    :param test_data: 测试集数据\n",
    "    :param save_model_path: 加载模型的路径和名称,请填写你认为最好的模型\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # ----------------------- 实现模型加载和评估等部分的代码 -----------------------\n",
    "    print('Please refer to project report for TensorBoard visualization result.')\n",
    "    model = garbage_classifier.model.resnet_50_pretrained((384, 512), 6)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(), \n",
    "        loss=keras.losses.categorical_crossentropy, \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.load_weights(save_model_path)\n",
    "\n",
    "    contingency_matrix = np.zeros((6, 6))\n",
    "    for images, labels in test_data:\n",
    "        for image, label in zip(images, labels):\n",
    "            contingency_matrix[np.argmax(label)][np.argmax(model.predict(np.array([image]))[0])] += 1\n",
    "    eps = np.finfo(float).eps\n",
    "    precisions = [contingency_matrix[i][i] / (np.sum(contingency_matrix[:, i]) + eps) for i in range(6)]\n",
    "    recalls = [contingency_matrix[i][i] / (np.sum(contingency_matrix[i, :]) + eps) for i in range(6)]\n",
    "    f1s = [2 * previsions[i] * recalls[i] / (precisions[i] + recalls[i] + eps) for i in range(6)]\n",
    "    for i in range(6):\n",
    "        print('class', i)\n",
    "        print('prevision', precisions[i])\n",
    "        print('recall', recalls[i])\n",
    "        print('F1', f1s[i])\n",
    "    # ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    深度学习模型训练流程,包含数据处理、创建模型、训练模型、模型保存、评价模型等。\n",
    "    如果对训练出来的模型不满意,你可以通过调整模型的参数等方法重新训练模型,直至训练出你满意的模型。\n",
    "    如果你对自己训练出来的模型非常满意,则可以提交作业!\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_path = './datasets/la1ji1fe1nle4ishu4ju4ji22-momodel/dataset-resized-full/'  # 数据集路径\n",
    "    save_model_path = './results/demo_model_weights.hdf5'  # 保存模型路径和名称\n",
    "\n",
    "    # 获取数据\n",
    "    train_data, test_data = processing_data(data_path)\n",
    "\n",
    "    # 创建、训练和保存模型\n",
    "    model(train_data, test_data, save_model_path)\n",
    "\n",
    "    # 评估模型\n",
    "    evaluate_mode(test_data, save_model_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 模型预测\n",
    "\n",
    "\n",
    "注意：\n",
    "1. 点击左侧栏`提交作业`后点击`生成文件`则只需勾选 `predict()` 函数的cell，即【**模型预测代码答题区域**】的 cell。\n",
    "2. 请导入必要的包和第三方库 (包括此文件中曾经导入过的)。\n",
    "3. 请加载你认为训练最佳的模型，即请按要求填写模型路径。\n",
    "4. `predict()`函数的输入和输出请不要改动。\n",
    "5. 作业测试时记得填写你的模型路径及名称, 如果采用 [离线任务](https://momodel.cn/docs/#/zh-cn/%E5%9C%A8GPU%E6%88%96CPU%E8%B5%84%E6%BA%90%E4%B8%8A%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B) 请将模型保存在 **results** 文件夹下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================  **模型预测代码答题区域**  ===========================================  \n",
    "在下方的代码块中编写 **模型预测** 部分的代码，请勿在别的位置作答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "select": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow import keras\n",
    "\n",
    "import garbage_classifier\n",
    "\n",
    "\n",
    "# -------------------------- 请加载您最满意的模型 ---------------------------\n",
    "# 加载模型(请加载你认为的最佳模型)\n",
    "# 加载模型,加载请注意 model_path 是相对路径, 与当前文件同级。\n",
    "# 如果你的模型是在 results 文件夹下的 dnn.h5 模型，则 model_path = 'results/dnn.h5'\n",
    "model_path = None\n",
    "\n",
    "# 加载模型，如果采用keras框架训练模型，则 model=load_model(model_path)\n",
    "model = garbage_classifier.model.resnet_50_pretrained((384, 512), 6)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(), \n",
    "    loss=keras.losses.categorical_crossentropy, \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.load_weights(model_path)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def predict(img):\n",
    "    \"\"\"\n",
    "    加载模型和模型预测\n",
    "    主要步骤:\n",
    "        1.图片处理\n",
    "        2.用加载的模型预测图片的类别\n",
    "    :param img: PIL.Image 对象\n",
    "    :return: string, 模型识别图片的类别, \n",
    "            共 'cardboard','glass','metal','paper','plastic','trash' 6 个类别\n",
    "    \"\"\"\n",
    "    # -------------------------- 实现模型预测部分的代码 ---------------------------\n",
    "    # 获取图片的类别，共 'cardboard','glass','metal','paper','plastic','trash' 6 个类别\n",
    "    # 把图片转换成为numpy数组\n",
    "    img = image.img_to_array(img)\n",
    "    \n",
    "    # 获取输入图片的类别\n",
    "    predict_lookup = {\n",
    "        0: 'cardboard',\n",
    "        1: 'glass',\n",
    "        2: 'metal',\n",
    "        3: 'paper',\n",
    "        4: 'plastic',\n",
    "        5: 'trash'\n",
    "    }\n",
    "    y_predict = predict_lookup[np.argmax(model.predict(np.array([img])))]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # 返回图片的类别\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "\n",
    "# 输入图片路径和名称\n",
    "img_path = 'test.jpg'\n",
    "\n",
    "# 打印该张图片的类别\n",
    "img = image.load_img(img_path)\n",
    "print(predict(img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}