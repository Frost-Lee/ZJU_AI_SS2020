{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics\n",
    "import h5py\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEAM_ARCHIVE_PATH = '/Users/canchel/Desktop/normalized_deam_samples.hdf5'\n",
    "\n",
    "TEST_SPLIT = 0.1\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with h5py.File(DEAM_ARCHIVE_PATH) as in_file:\n",
    "    song_ids = sorted(map(int, in_file.keys()))\n",
    "    feature_dict = {song_id : np.array(in_file[str(song_id) + '/features']) for song_id in song_ids}\n",
    "    valence_dict = {song_id : np.array(in_file[str(song_id) + '/valence']) for song_id in song_ids}\n",
    "    arousal_dict = {song_id : np.array(in_file[str(song_id) + '/arousal']) for song_id in song_ids}\n",
    "\n",
    "random.shuffle(song_ids)\n",
    "split_1, split_2 = math.ceil((1.0 - VALIDATION_SPLIT - TEST_SPLIT) * len(song_ids)), math.ceil((1 - TEST_SPLIT) * len(song_ids))\n",
    "training_song_ids, validation_song_ids, test_song_ids = song_ids[:split_1], song_ids[split_1:split_2], song_ids[split_2:]\n",
    "\n",
    "def get_discrete_samples(song_ids):\n",
    "    feature_matrix = np.concatenate([feature_dict[song_id] for song_id in song_ids], axis=0)\n",
    "    label_matrix = np.concatenate([\n",
    "        np.concatenate([valence_dict[song_id] for song_id in song_ids], axis=0),\n",
    "        np.concatenate([arousal_dict[song_id] for song_id in song_ids], axis=0)\n",
    "    ], axis=1)\n",
    "    return feature_matrix, label_matrix\n",
    "\n",
    "def get_sequential_samples(song_ids, clip_range=(8, 60), batch_size=4):\n",
    "    def clip(feature_sequence, label_sequence, length):\n",
    "        assert len(feature_sequence) == len(label_sequence)\n",
    "        assert length <= len(feature_sequence)\n",
    "        start_index = np.random.randint(0, len(feature_sequence) - length + 1)\n",
    "        return feature_sequence[start_index : start_index + length], label_sequence[start_index : start_index + length]\n",
    "    dict_indexer = lambda x, y: [x[i] for i in y]\n",
    "    features, valences, arousals = tuple(map(lambda x: dict_indexer(x, song_ids), [feature_dict, valence_dict, arousal_dict]))\n",
    "    candidate_pool = [*zip(features, valences, arousals)]\n",
    "    while True:\n",
    "        clip_length = np.random.randint(*clip_range)\n",
    "        selected_samples = random.choices(candidate_pool, k=batch_size)\n",
    "        clips = [clip(feature_seq, np.concatenate([valence_seq, arousal_seq], axis=-1), clip_length) for feature_seq, valence_seq, arousal_seq in selected_samples]\n",
    "        yield tuple(map(np.array, [*zip(*clips)]))\n",
    "\n",
    "\n",
    "def evaluate(song_ids, evaluators, evaluator_names=None):\n",
    "    assert evaluator_names is None or len(evaluators) == len(evaluator_names)\n",
    "    metric_calculator_dict = {\n",
    "        'mse' : sklearn.metrics.mean_squared_error,\n",
    "        'r2' : sklearn.metrics.r2_score\n",
    "    }\n",
    "    labels = []\n",
    "    predictions = [[] for _ in range(len(evaluators))]\n",
    "    for step_index in range(len(song_ids)):\n",
    "        feature, label = [*map(lambda x: x[0], next(get_sequential_samples(song_ids, clip_range=(8, 9), batch_size=1)))]\n",
    "        labels += list(label)\n",
    "        for evaluator_index in range(len(evaluators)):\n",
    "            predictions[evaluator_index] += list(evaluators[evaluator_index](feature))\n",
    "    labels, predictions = np.array(labels), [np.array(p) for p in predictions]\n",
    "    for metric_name in metric_calculator_dict.keys():\n",
    "        print(metric_name)\n",
    "        for evaluator_index in range(len(evaluators)):\n",
    "            print(evaluator_names[evaluator_index] if evaluator_names is not None else 'evaluator ' + str(evaluator_index), end=': ')\n",
    "            for label_index in range(labels.shape[-1]):\n",
    "                print(metric_calculator_dict[metric_name](labels[:, label_index], predictions[evaluator_index][:, label_index]), end=' ')\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model_input = tf.keras.layers.Input(shape=(130,))\n",
    "layer = tf.keras.layers.Dense(\n",
    "    units=32,\n",
    "    activation='relu',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    ")(mlp_model_input)\n",
    "for _ in range(4):\n",
    "    layer = tf.keras.layers.Dense(\n",
    "        units=32,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "    )(layer)\n",
    "layer = tf.keras.layers.Dense(\n",
    "    units=2,\n",
    "    activation='tanh',\n",
    "    kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    ")(layer)\n",
    "\n",
    "mlp_model = tf.keras.models.Model(inputs=mlp_model_input, outputs=layer)\n",
    "mlp_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.MSE\n",
    ")\n",
    "\n",
    "def mlp_evaluator(feature_sequence):\n",
    "    return mlp_model.predict(feature_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp_model.fit(\n",
    "    *get_discrete_samples(training_song_ids), \n",
    "    epochs=16, \n",
    "    batch_size=32, \n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.LearningRateScheduler(lambda ei, lr: lr / 2.0 if ei > 0 and (ei / 2).is_integer() else lr)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq2seq_model_input = tf.keras.layers.Input(shape=(8, 130))\n",
    "layer = tf.keras.layers.LSTM(32, return_sequences=False)(seq2seq_model_input)\n",
    "layer = tf.keras.layers.Dense(32, activation='relu')(layer)\n",
    "layer = tf.keras.layers.RepeatVector(8)(layer)\n",
    "layer = tf.keras.layers.LSTM(32, return_sequences=True)(layer)\n",
    "layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(2, activation='tanh'))(layer)\n",
    "seq2seq_model = tf.keras.models.Model(inputs=seq2seq_model_input, outputs=layer)\n",
    "seq2seq_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.MSE\n",
    ")\n",
    "\n",
    "def seq2seq_evaluator(feature_sequence):\n",
    "    return seq2seq_model.predict(np.array([feature_sequence]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq2seq_model.fit(\n",
    "    get_sequential_samples(training_song_ids, clip_range=(8, 9)), \n",
    "    epochs=16, \n",
    "    steps_per_epoch=len(training_song_ids),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.LearningRateScheduler(lambda ei, lr: lr / 2.0 if ei > 0 and (ei / 2).is_integer() else lr)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shared_layer_cache = {}\n",
    "seq2seq_attention_model_input = tf.keras.layers.Input(shape=(8, 130))\n",
    "encoder_output, encoder_state = tf.keras.layers.GRU(32, return_sequences=True, return_state=True)(seq2seq_attention_model_input)\n",
    "\n",
    "def attention_decoder_layer():\n",
    "    def structure(previous_output, encoder_state, encoder_output):\n",
    "        global shared_layer_cache\n",
    "        encoder_state = tf.keras.backend.expand_dims(encoder_state, axis=1)\n",
    "        if 'attention_decoder_layer' not in shared_layer_cache:\n",
    "            shared_layer_cache['attention_decoder_layer'] = {\n",
    "                'encoder_state_weight_layer_1' : tf.keras.layers.Dense(32),\n",
    "                'encoder_output_weight_layer_1' : tf.keras.layers.Dense(32),\n",
    "                'score_weight_layer_1' : tf.keras.layers.Dense(1),\n",
    "                'output_gru_layer_1' : tf.keras.layers.GRU(32, return_sequences=True, return_state=True),\n",
    "                'output_dense_layer_1' : tf.keras.layers.Dense(2, activation='tanh')\n",
    "            }\n",
    "        cache = shared_layer_cache['attention_decoder_layer']\n",
    "        score = cache['encoder_state_weight_layer_1'](encoder_state) + cache['encoder_output_weight_layer_1'](encoder_output)\n",
    "        score = cache['score_weight_layer_1'](tf.keras.layers.Activation('tanh')(score))\n",
    "        attention_weights = tf.keras.layers.Softmax(axis=1)(score)\n",
    "        context_vector = tf.keras.backend.sum(attention_weights * encoder_output, axis=1)\n",
    "        context_concat = tf.keras.layers.Concatenate(axis=-1)([previous_output, tf.keras.backend.expand_dims(context_vector, axis=1)])\n",
    "        output, decoder_state = cache['output_gru_layer_1'](context_concat)\n",
    "        output = cache['output_dense_layer_1'](output)\n",
    "        return output, decoder_state, attention_weights\n",
    "    return structure\n",
    "\n",
    "previous_output = tf.keras.backend.zeros((4, 1, 2))\n",
    "decoder_state = encoder_state\n",
    "outputs = []\n",
    "for _ in range(8):\n",
    "    previous_output, decoder_state, _ = attention_decoder_layer()(previous_output, decoder_state, encoder_output)\n",
    "    outputs.append(previous_output)\n",
    "final_output = tf.keras.layers.Concatenate(axis=1)(outputs)\n",
    "seq2seq_attention_model = tf.keras.models.Model(inputs=seq2seq_attention_model_input, outputs=final_output)\n",
    "\n",
    "seq2seq_attention_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.MSE\n",
    ")\n",
    "\n",
    "def seq2seq_attention_evaluator(feature_sequence):\n",
    "    return seq2seq_attention_model.predict(np.array([feature_sequence] * 4))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq2seq_attention_model.fit(\n",
    "    get_sequential_samples(training_song_ids, clip_range=(8, 9)), \n",
    "    epochs=16, \n",
    "    steps_per_epoch=len(training_song_ids),\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.LearningRateScheduler(lambda ei, lr: lr / 2.0 if ei > 0 and (ei / 2).is_integer() else lr)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate(test_song_ids, [mlp_evaluator, seq2seq_evaluator, seq2seq_attention_evaluator], ['MLP', 'seq2seq', 'seq2seq_attention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitd9f33c60cf0741e28b54c6db950adecd",
   "display_name": "Python 3.7.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}