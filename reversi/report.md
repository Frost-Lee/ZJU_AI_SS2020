# 创造不借鉴人类知识的翻转棋 AI

### 模型测试

虽然我们描述了我们训练 reversi-zero 的具体过程，但是，在实际操作过程中，这些过程的确定并不是一蹴而就，而是在探索中确定的。因此，在试错的过程中，我们训练了多个版本的 reversi-zero 模型，在其中，我们选出了 4 个较为有代表性的模型：

- rz-raw：使用 8 个残差块，未经训练的 reversi-zero 模型
- rz-serial-1：使用 8 个残差块，在训练中不断 debug，每次更改代码之后暂停训练，再使用新的代码加载之前训练过的模型继续训练
- rz-serial-2：使用 8 个残差块，在代码完善之后从头训练
- rz-full：使用 16 个残差块，将之前产生的所有训练数据汇总，在所有的训练数据上重新训练

我们在温度为 0.05 的前提下对这些模型进行了评判。每两个模型都将对弈 16 次，双方执黑 / 执白数量相等，最终的对弈结果如下。由于模型本身的优化目标是赢下棋局，而不是赢的多少，因此我们并没有记录最终的分数，而是只记录了棋局的胜负。注意在如下表格中，每个格子的胜负是相对于行标题而言的。

|                 |     rz-raw      |   rz-serial-1   |   rz-serial-2   |     rz-full     |
| :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
|   **rz-raw**    | 胜 7 平 0 负 9  | 胜 1 平 1 负 14 | 胜 2 平 0 负 14 | 胜 0 平 0 负 16 |
| **rz-serial-1** | 胜 14 平 1 负 1 | 胜 6 平 0 负 10 | 胜 6 平 0 负 10 | 胜 0 平 0 负 16 |
| **rz-serial-2** | 胜 14 平 1 负 1 | 胜 10 平 0 负 6 | 胜 9 平 0 负 7  | 胜 1 平 0 负 15 |
|   **rz-full**   | 胜 16 平 0 负 0 | 胜 16 平 0 负 0 | 胜 15 平 0 负 1 | 胜 6 平 1 负 9  |

从结果中可以看出，rz-full 在所有模型中拥有最好的表现，因此，在本项目的最终提交版本中，神经网络模型采用了 rz-full。