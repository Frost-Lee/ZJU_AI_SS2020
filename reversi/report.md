# 创造不借鉴人类知识的黑白棋程序

## 摘要

在本项目中，我们基于 AlphaGo Zero 算法，结合蒙特卡洛树搜索（MCTS）与深度神经网络，训练了用于进行黑白棋的人工智能（AI）程序。在测试中，我们的 AI 在与人和 Mo 平台的测试程序的对弈中均有较高的胜率。考虑到训练成本的问题，本项目在很多地方对 AlphaGo Zero 的算法进行了简化。参考 AlphaGo Zero 的名称，我们将模型命名为 reversi-zero。

## 模型架构

reversi-zero 基于 AlphaGo Zero 算法，即在蒙特卡洛树搜索的过程中使用深度神经网络作为为每个非叶节点进行价值评估的 default policy，并在选择节点进行分裂操作时考虑到节点由深度神经网络预测的先验概率，最终使用当前状态经过 softmax 操作的子节点访问次数作为当前状态的 policy。

### 蒙特卡洛树搜索

蒙特卡洛树是 reversi-zero 运行的基础架构。在基础的蒙特卡洛树中，每个节点代表一个棋盘状态，每条边代表一个落子行为。在每条边上记录着关于对应落子行为与价值评估相关的各类参数，搜索算法会根据这些价值参数选定需要分裂的子节点，在有限的时间内尽量多地便利棋局中可能的高价值发展趋势，并从中找出制胜手段。

为了数据结构的简便起见，reversi-zero 将边和节点的信息统一归纳到 `mct.MCTNode` 中，具体的信息包括了对应状态被访问的次数、转移到此状态的最后一步落子颜色，此状态可行的落子位置以及对于将要落子的棋手而言，此状态被评估的价值之和以及每一个落子位置的先验概率。在每一次落子中，reversi-zero 会执行 `config.MCT_SIMULATION_COUNT`（对于用于训练的版本是 128，对于提交的版本是 256）次选择 - 分裂 - 预测 - 反向传播的操作。每一次操作的伪代码如下。

``` pseudocode
sub_root = root
while sub_root.children is not null:
	sub_root = get_hightest_value_children(root)
if is_terminal_state(sub_root):
	policy, value = null, is_win(sub_root, root.player) ? 1 : -1
else:
	policy, value = model.predict(sub_root.state)
sub_root.init_children(policy)
back_propargate(sub_root, value)
```

#### 棋盘状态表示

在 AlphaGo Zero 论文中，作者使用 1 表示有子，0 表示无子，从而将棋盘中单个颜色的落子信息视为一个 19 * 19 的矩阵。在状态表示中，作者将当前的棋盘以及之前 7 步的黑棋 / 白棋的落子矩阵棋盘，并使用一个额外的维度来表示此状态下将要落子的一方，从而使用一个 19 * 19 * 17 的张量来表示当前的期盼状态。考虑到训练成本，reversi-zero 取消了表示棋盘状态的张量中描述之前棋盘格局的部分以及表示此状态下将要落子一方的部分，而是使用黑棋 / 白棋的落子矩阵的叠加顺序来隐形地描述将要落子的一方。因此，reversi-zero 的期盼状态使用 8 * 8 * 2 的矩阵来表示，状态的具体计算方法参考 `utils.board_state`。

#### 子节点的选择

与 AlphaGo Zero 论文中的描述一致，reversi-zero 在每一次模拟中选取最大 upper confidence bound (UCB) 值最大的子节点前进。其中 UCB 的计算方法如下：
$$
Q(s) = \frac{\sum_{s' | s, a \rightarrow s'} V(s')}{N(s)} \\
U(s) = c_{puct} P(s) \frac{\sqrt{N(s')_{s' | s', a \rightarrow s}}}{1 + N(s)} \\
UCB(s) = Q(s) + U(s)
$$
其中 $V(s), N(s), P(s)$ 分别表示 $s$ 状态对应的节点被估计的价值、被访问的次数以及被赋予的先验概率。$c_{puct}$ 为用于调整 exploration 与 exploitation 权重的超参数。根据[此项目](https://github.com/aqtq314/AlphaZero)中的选择，我们将 $c_{puct}$ 的值选为 `np.sqrt(config.MCT_SIMULATION_COUNT) / 8`。

#### 落子决策

经过多次选择 - 分裂 - 预测 - 反向传播的操作之后，蒙特卡洛树中跟节点的每个子节点都具有对应的访问次数，因此我们可以直接将对应节点的访问次数经过 softmax 函数操作，给出一个基础的 policy，即 $\pi_i \propto N(s_i)$。而为了增加更多的随机性，从而增大训练数据对棋局情形的覆盖面，根据 AlphaGo Zero 论文中的做法，我们引入了温度常数 $\tau$，并将 policy 的计算更改为 $\pi_i \propto N(s_i)^\frac{1}{\tau}$。在获取了根节点的 policy 之后，reversi-zero 以 policy 为概率分布，随机选择决定落子位置。

### 深度神经网络

在蒙特卡洛树搜索的基础上，深度神经网络为 reversi-zero 提供了更为优质的状态价值估计以及落子先验概率估计，从而让蒙特卡洛树搜索更加有效率。深度神经网络模型的输入为当前的棋盘状态张量，输出为当前局面的价值（value，对于最后一位落子的棋手而言）以及之后落子的先验概率分布（policy，对于将要落子的棋手而言）。在训练中，我们使用 8 层的类残差块结构（加深输入张量的深度而不缩减面积）作为深度神经网络的特征提取部分，并在提取的特征后加上不同的全连接层来输出 value 与 policy。关于神经网络的具体结构，请参考 `nn_model.NNModel._create_model`。

## 模型训练

模型的训练过程指的是深度神经网络部分的训练，此过程旨在让深度神经网络为蒙特卡洛树搜索提供更有价值的 value 与 policy 估计。由于 reversi-zero 并不借鉴任何人类知识，因此训练的过程并不借助已有的数据集，而是使用自对弈生成的数据集进行。模型使用的损失函数如下：
$$
l = (z - v)^2 - \pi^T \log p + c \Vert \theta \Vert^2
$$

#### 自对弈

自对弈可以为 reversi-zero 累计黑白棋的对弈数据，从而获得模型的输入和预期的输出。在进行训练之后，新的模型被重新用来生成下一轮的训练数据，反复迭代，不断提高。具体而言，在每次自对弈过程中，两个 reversi-zero 依次落子，每次落子时产生的棋盘状态和落子者给出的 policy 作为模型训练的输入与 policy 输出。而模型的 value 输出则依赖于棋局的最终结果：在棋局结束后，根据落子顺序，获胜者的每一步 value 为 1，失败者的每一步 value 为 -1。

为了在自对弈中增大数据覆盖的棋局多样性，根据 AlphaGo Zero 论文以及我们项目的具体情况，我们采取了以下两种措施。

1. 为模型预测的 policy 添加权重为 0.25 的 Dirichlet 噪声
2. 将温度常数 $\tau$ 在自对弈的前 16 步设置为 1，之后设置为 0.01

在每一轮训练结尾，新的模型会与之前的模型在温度为 0.05 的情况下进行 5 局对弈，若新的模型获胜 3 次及以上，新的模型将会被用来生成新的数据，否则依然用之前的模型生成新的数据。

#### 训练方式

在模型的训练中，我们每进行 32 次自对弈对模型进行一轮训练。每一轮训练中，我们使用之前 4 轮自对弈产生的数据训练 32 个 epoch。在训练中我们使用 SGD 优化器，固定学习率为 $10^{-4}$，动量为 0.9。对于训练数据，我们使用旋转 / 对称的方法进行数据增强，因此每个样本可以产生 7 个训练数据。

我们的训练在 Windows 平台，使用 NVIDIA GTX 2060 GPU 进行，整个训练过程约 3 天。最终我们获得了 3,442,138 个训练样本（未经过数据增强）。

> 特别地，我们在所有训练结束之后汇总了训练中产生的全部数据，并使用全部数据训练一个具有 16 个类残差块结构的深度神经网络模型。此训练中我们使用了 Adam 优化器，初始的学习率为 $10^{-3}$，每 16 个 epoch 将学习率减半，共训练 64 个 epoch。虽然此模型最终获得的 loss 远高于上文提到的训练方式获得的模型，但此模型的实际表现好于之前的所有模型，具体表现见模型测试部分。此模型在 Google Colab 上训练，耗时大约 15 小时。

## 模型测试

虽然我们描述了我们训练 reversi-zero 的具体过程，但是，在实际操作过程中，这些过程的确定并不是一蹴而就，而是在探索中确定的。因此，在试错的过程中，我们训练了多个版本的 reversi-zero 模型，在其中，我们选出了 4 个较为有代表性的模型：

- rz-raw：使用 8 个类残差块，未经训练的 reversi-zero 模型
- rz-serial-1：使用 8 个类残差块，在训练中不断 debug，每次更改代码之后暂停训练，再使用新的代码加载之前训练过的模型继续训练
- rz-serial-2：使用 8 个类残差块，在代码完善之后从头训练
- rz-full：使用 16 个类残差块，将之前产生的所有训练数据汇总，在所有的训练数据上重新训练

我们在温度为 0.05 的前提下对这些模型进行了评判。每两个模型都将对弈 16 次，双方执黑 / 执白数量相等，最终的对弈结果如下。由于模型本身的优化目标是赢下棋局，而不是赢的多少，因此我们并没有记录最终的分数，而是只记录了棋局的胜负。注意在如下表格中，每个格子的胜负是相对于行标题而言的。

|                 |     rz-raw      |   rz-serial-1   |   rz-serial-2   |     rz-full     |
| :-------------: | :-------------: | :-------------: | :-------------: | :-------------: |
|   **rz-raw**    | 胜 7 平 0 负 9  | 胜 1 平 1 负 14 | 胜 2 平 0 负 14 | 胜 0 平 0 负 16 |
| **rz-serial-1** | 胜 14 平 1 负 1 | 胜 6 平 0 负 10 | 胜 6 平 0 负 10 | 胜 0 平 0 负 16 |
| **rz-serial-2** | 胜 14 平 0 负 2 | 胜 10 平 0 负 6 | 胜 9 平 0 负 7  | 胜 1 平 0 负 15 |
|   **rz-full**   | 胜 16 平 0 负 0 | 胜 16 平 0 负 0 | 胜 15 平 0 负 1 | 胜 6 平 1 负 9  |

从结果中可以看出，rz-full 在所有模型中拥有最好的表现，因此，在本项目的最终提交版本中，神经网络模型采用了 rz-full。对于 Mo 平台给出的简单 / 中等 / 困难的测试程序，目前提交的棋局中 reversi-zero 均获得胜利。

## 结论与见解

目前来看，reversi-zero 取得了较为不错的对弈结果，包括对人类玩家、Mo 平台的测试程序以及未经训练的 reversi-zero，这些成果充分说明了深度神经网络对于蒙特卡洛树搜索的辅助作用。但是，由于我们并未参与之前举行的挑战赛，reversi-zero 的具体表现对于我们仍然是个未知数。

事实上，reversi-zero 是在带有严重错误的情况下进行训练的：我们在之前的部分提到过，训练过的模型在 5 局 3 胜的比赛中击败之前的模型才能够用来生成新的数据。在相应的评估函数中，新老模型不断切换黑白执子，但我们在分数评估时错误地默认新模型执黑子，因此算法采纳的新模型并不一定获得了 3 场及以上的胜利。由于发现这个错误时已经临近项目期限，我们并没有时间在修正错误后重新训练，也并没有办法预估这个错误会对模型造成多大的性能影响。

state、policy 以及 value 的相互匹配问题是至关重要的，例如在深度神经网络的训练数据中，训练数据中的 value 是针对最后一位落子者计算的 value，而在蒙特卡洛树搜索中扩展子节点时，需要的 value 是针对将要落子者的 value，两者正好相反，再加上黑白棋的规则中并不一定是黑白双方轮流落子，因此我们花了一定的时间才意识到这个问题，并给出了合适的解决方案。

rz-full 取得的性能表现说明了我们使用的训练方式（使用前 4 轮生成的数据训练模型）存在一定的问题。在 AlphaGo Zero 论文中，此处的对策是从最近 500,000 次自对弈中随机选取训练数据。我们的训练方式可能较为缺乏多样性，使 reversi-zero “沉浸在自己的世界”，而忘记黑白棋存在的更多可能性。

最后，在不同平台的训练导致了加载 Keras 模型出现了一些问题。用于训练的 Windows 平台的 TensorFlow 版本为 1.12，Google Colab 的 TensorFlow 版本为 2.2，而用于测试的 Mo 平台 TensorFlow 版本为 1.13，高版本的 Keras 模型无法在低版本上整体加载。因此在 Mo 平台的提交中，我们加载模型权重而不是模型本身，使用的模型加载代码也与 master 分支的代码有所不同。